{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Excercise 1.1\n",
      "First sentence pair of Training set:  \n",
      "0.16 Where in th world is Rn Paul? Where ĺո ɩhe ωorld îs the MEDIA??\n",
      "\n",
      "Done writing\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "#             1.1 Data Formats\n",
    "# ------------------------------------------------\n",
    "\n",
    "####################################\n",
    "def read_data(filename, labeled = True ):\n",
    "    list_scores = []\n",
    "    list_first_sent = []\n",
    "    list_second_sent = []\n",
    "    with open(filename,'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            content = line.strip(\" \").split('\\t')\n",
    "            list_scores.append(float(content[0]))\n",
    "            list_first_sent.append(content[1])\n",
    "            list_second_sent.append(content[2])\n",
    "    if labeled:\n",
    "        return list_scores, list_first_sent, list_second_sent\n",
    "    else:\n",
    "        return list_first_sent, list_second_sent\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Excercise 1.1\")\n",
    "print(\"First sentence pair of Training set:  \")\n",
    "data = read_data(\"data-test.txt\")\n",
    "print(data[0][0], data[1][0],data[2][0])\n",
    "\n",
    "\n",
    "def write_simscore(filename, sim_list):\n",
    "    with open(filename, 'w') as f:\n",
    "        for element in sim_list:\n",
    "            f.write(str(element))\n",
    "            f.write(\"\\n\")\n",
    "        f.close()\n",
    "    print(\"Done writing\")\n",
    "\n",
    "write_simscore(\"simscores.txt\",data[0])\n",
    "print(\"----------------------------------------------\")\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39999\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "#             1.2 Embedding the Sentences\n",
    "# ------------------------------------------------\n",
    "# Expect the embeddings to be accessible at ./wiki-news-300d-1M.vec here, but\n",
    "# DO NOT upload them to Moodle!\n",
    "\n",
    "####################################\n",
    "import  numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def read_wiki_dic(filename, limit=10000):\n",
    "    wiki_dic = {}\n",
    "    try:\n",
    "        with open(filename, 'r',encoding='utf8') as f:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                if counter == 0:\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                element = line.strip('\\n').split(' ')\n",
    "                word = element[0]\n",
    "                vec  = np.float_(element[1:])\n",
    "                wiki_dic[word] = vec\n",
    "                counter += 1\n",
    "                if counter >= limit+1:\n",
    "                    break\n",
    "            f.close()\n",
    "    except:\n",
    "        print(\"Could not read vocab in position {}\".format(counter))\n",
    "    return wiki_dic\n",
    "wik_dic = read_wiki_dic('wiki-news-300d-1M.vec',limit=40000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized first sentence of training data set:\n",
      "['A', 'brown', 'dog', 'is', 'running', 'through', 'the', 'field', '.']\n",
      "\n",
      "First 20 dimensions of averaged vector:\n",
      "[-0.04035556  0.04834444 -0.02134444  0.05437778 -0.04855556 -0.02447778\n",
      " -0.02275556 -0.00113333  0.00976667  0.01081111  0.03703333  0.01817778\n",
      "  0.05385556  0.05241111  0.03544444 -0.00903333  0.02505556  0.04105556\n",
      " -0.02614444  0.01518889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1.2 b)\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "example_sentence = tokenize(read_data(\"data-train.txt\")[1][0])\n",
    "\n",
    "print(\"Tokenized first sentence of training data set:\")\n",
    "print(example_sentence)\n",
    "print()\n",
    "\n",
    "# 1.2 c)\n",
    "def token_to_vector(token_list):\n",
    "    vector_list = []\n",
    "    for token in token_list:\n",
    "        try:\n",
    "            vector = wik_dic[token]\n",
    "        except:\n",
    "            print(\"No words found for {}. Assign zero-vector.\".format(token))\n",
    "            vector = np.zeros(300)\n",
    "        vector_list.append(vector)\n",
    "    return vector_list\n",
    "\n",
    "# 1.3 d)\n",
    "def  embed_sentence(vector_list):\n",
    "    sum_vector = np.zeros(300)\n",
    "    for vec in vector_list:\n",
    "        sum_vector += vec\n",
    "    return np.divide(sum_vector, len(vector_list))\n",
    "\n",
    "print(\"First 20 dimensions of averaged vector:\")\n",
    "print(embed_sentence(token_to_vector(example_sentence))[:20])\n",
    "####################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "#             1.3 Scoring the Similarity\n",
    "# ------------------------------------------------\n",
    "\n",
    "####################################\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Activation, Dense, Input, Dropout\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# CHeck if GPU is there\n",
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_device))\n",
    "tf.config.experimental.set_memory_growth(physical_device[0],True)\n",
    "\n",
    "# Create model\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(300,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "####################################\n",
    "\n",
    "# Train model!\n",
    "\n",
    "#model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.fit(x=, y=,  validation_split=0.1, batch_size=10, epochs=30, shuffle=True, verbose=2 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-29-bb1d9500f868>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0msecond\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m300\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'sigmoid'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mmerged\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfirst\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecond\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py\u001B[0m in \u001B[0;36mconcatenate\u001B[1;34m(inputs, axis, **kwargs)\u001B[0m\n\u001B[0;32m    943\u001B[0m       \u001B[0mA\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mconcatenation\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0minputs\u001B[0m \u001B[0malongside\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    944\u001B[0m   \"\"\"\n\u001B[1;32m--> 945\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mConcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    946\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    947\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1021\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname_scope_v2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1022\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1023\u001B[1;33m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_build\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1024\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1025\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_autocast\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m_maybe_build\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   2623\u001B[0m         \u001B[1;31m# operations.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2624\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaybe_init_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2625\u001B[1;33m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shapes\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint:disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2626\u001B[0m       \u001B[1;31m# We must set also ensure that the layer is marked as built, and the build\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2627\u001B[0m       \u001B[1;31m# shape is stored since user defined build functions may not be calling\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(instance, input_shape)\u001B[0m\n\u001B[0;32m    268\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0minput_shape\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    269\u001B[0m       \u001B[0minput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_shapes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mto_tuples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 270\u001B[1;33m     \u001B[0moutput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    271\u001B[0m     \u001B[1;31m# Return shapes from `fn` as TensorShapes.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    272\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0moutput_shape\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py\u001B[0m in \u001B[0;36mbuild\u001B[1;34m(self, input_shape)\u001B[0m\n\u001B[0;32m    487\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    488\u001B[0m     \u001B[1;31m# Used purely for shape validation.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 489\u001B[1;33m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    490\u001B[0m       raise ValueError('A `Concatenate` layer should be called '\n\u001B[0;32m    491\u001B[0m                        'on a list of at least 1 input.')\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "''' class ModelNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(selfself):\n",
    "        super(ModelNN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(2, activation=tf.nn.relu) '''\n",
    "\n",
    "first = Sequential()\n",
    "first.add(Dense(1, input_shape=(300,), activation='sigmoid'))\n",
    "\n",
    "second = Sequential()\n",
    "second.add(Dense(1, input_shape=(300,), activation='sigmoid'))\n",
    "\n",
    "merged = keras.layers.concatenate([first, second])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model_input = keras.models.Model(inputs = [first_input,second_input], outputs= merged_input)\n",
    "\n",
    "model\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "#Last Layer\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-36-9806625728e3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDense\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconcatenate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mx_in\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"x_in\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0m__future__\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mabsolute_import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mutils\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mactivations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mapplications\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\utils\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mnp_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mto_categorical\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mnp_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnormalize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mmulti_gpu_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmulti_gpu_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\utils\\multi_gpu_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0m__future__\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mprint_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmerge\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconcatenate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLambda\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\layers\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeneric_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeserialize_keras_object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbase_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLayer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInputLayer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\engine\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# note: `Node` is an internal class,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m# it isn't meant to be used by Keras users.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0minput_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0minput_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInputLayer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbase_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInputSpec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\engine\\input_layer.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0m__future__\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdivision\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbase_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLayer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbase_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mNode\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbackend\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minitializers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcount_params\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeneric_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mhas_arg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\initializers\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[1;31m# from ALL_OBJECTS. We make no guarantees as to whether these objects will\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[1;31m# using their correct version.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 124\u001B[1;33m \u001B[0mpopulate_deserializable_objects\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    125\u001B[0m \u001B[0mglobals\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLOCAL\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mALL_OBJECTS\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\pyhton_verzeichnis\\bib_daten\\lib\\site-packages\\keras\\initializers\\__init__.py\u001B[0m in \u001B[0;36mpopulate_deserializable_objects\u001B[1;34m()\u001B[0m\n\u001B[0;32m     80\u001B[0m     \u001B[0mv2_objs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[0mbase_cls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minitializers_v2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInitializer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 82\u001B[1;33m     generic_utils.populate_dict_with_module_objects(\n\u001B[0m\u001B[0;32m     83\u001B[0m         \u001B[0mv2_objs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0minitializers_v2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "x_in = Input(shape=(100,), name = \"x_in\")\n",
    "y_in = Input(shape=(100,), name = \"y_in\")\n",
    "\n",
    "x = Dense(64, activation='relu')(x_in)\n",
    "y = Dense(64, activation='relu')(y_in)\n",
    "\n",
    "z = concatenate(x,y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}